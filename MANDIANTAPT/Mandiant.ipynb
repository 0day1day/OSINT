{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "APT-Maxmind-Enrichment-Product-2013-07-14-09-25-42.csv\t   get_fqdns.py  makmind2_test.py\tMandiant.ipynb\t\toutput\t      spitball.py\r\n",
        "APT-VirusTotal-Enrichment-Product-2013-07-14-21-37-16.csv  __init__.py\t mandiant_apt_list.txt\tnetworkx_clustering.py\tprobe_apt.py\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load spitball.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__date__ = \"03082014\"\n",
      "__author__ = \"AlienOne\"\n",
      "__copyright__ = \"GPL\"\n",
      "__credits__ = [\"AlienOne\"]\n",
      "__license__ = \"GPL\"\n",
      "__version__ = \"0.0.7\"\n",
      "__maintainer__ = \"AlienOne\"\n",
      "__email__ = \"ali3n0ne@alienone.org\"\n",
      "__status__ = \"Prototype\"\n",
      "\n",
      "\n",
      "import requests\n",
      "import csv\n",
      "from pandas import DataFrame\n",
      "\n",
      "\n",
      "def getData():\n",
      "    \"\"\"Get CSV Mandiant Data Set - output list data structure\"\"\"\n",
      "    response = requests.get(\"https://raw.github.com/alienone/OSINT/master/MANDIANTAPT/APT-Maxmind-Enrichment-Product-2013-07-14-09-25-42.csv\")\n",
      "    iterResponse = response.iter_lines()\n",
      "    next(iterResponse)\n",
      "    for line in iterResponse:\n",
      "        yield line.split(',')\n",
      "\n",
      "\n",
      "def create_nodes(args=list):\n",
      "    \"\"\"Create list of dicts - each dict will represent a node\"\"\"\n",
      "    list_dicts = []\n",
      "    for prodList in getData():\n",
      "        if len(prodList[0]) != 0:\n",
      "            dict_object = dict(zip(args, prodList))\n",
      "            list_dicts.append(dict_object)\n",
      "    return list_dicts\n",
      "\n",
      "\n",
      "def clusterData(args, column_name):\n",
      "    \"\"\"Leverage Panadas Groupby to cluster nodes by ASN - Then convert resulting DataFrames into list dicts by cluster\"\"\"\n",
      "    gd = DataFrame(create_nodes(args)).groupby(column_name)\n",
      "    asn_groups = [x[0] for x in gd]\n",
      "    for asn in asn_groups:\n",
      "        df = gd.get_group(asn)\n",
      "        create_dict = [{k: df.values[i][v] for v, k in enumerate(df.columns)} for i in range(len(df))]\n",
      "        yield create_dict\n",
      "\n",
      "\n",
      "def normalize_cluster(cluster):\n",
      "    \"\"\"Return dicitonary object with value lists\"\"\"\n",
      "    normalized_cluster = {'ASN': [], 'FQDN': []}\n",
      "    del normalized_cluster['ASN'][:]\n",
      "    del normalized_cluster['FQDN'][:]\n",
      "    for node in cluster:\n",
      "        for k, v in node.items():\n",
      "            normalized_cluster[k].append(v)\n",
      "    return normalized_cluster\n",
      "\n",
      "\n",
      "def get_attributes(asn_str, csv_file):\n",
      "    \"\"\"Lookup additional attributes to add to final product\"\"\"\n",
      "    with open(csv_file, 'rb') as fh:\n",
      "        for item in fh:\n",
      "            data = item.split(',')\n",
      "            if asn_str in data:\n",
      "                    asn_ip = data[2]\n",
      "                    lat_lon = data[8:10]\n",
      "                    country = data[4:5]\n",
      "                    return lat_lon, country, asn_ip\n",
      "\n",
      "\n",
      "def normalized_nodes(csv_file):\n",
      "    \"\"\"Remove duplicates, add additional attributes\"\"\"\n",
      "    results_list = []\n",
      "    args = [\"FQDN\", \"ASN\"]\n",
      "    column_name = 'ASN'\n",
      "    for cluster in clusterData(args, column_name):\n",
      "        results_list.append(normalize_cluster(cluster))\n",
      "    for node in results_list:\n",
      "        asn_str = list(set(node['ASN']))[0]\n",
      "        keys = ['ASN', 'ASN IP', 'FQDN', 'LatLon', 'Locale']\n",
      "        values = list(set(node['ASN']))[0], get_attributes(asn_str, csv_file)[2], list(set(node['FQDN'])), \\\n",
      "        get_attributes(asn_str, csv_file)[0], \\\n",
      "                 get_attributes(asn_str, csv_file)[1][0]\n",
      "        dict_obj = dict(zip(keys, values))\n",
      "        yield dict_obj\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Display nodes and write Country, Latitude/Longitude to CSV for D3.js World Map Display\"\"\"\n",
      "    csv_file = \"APT-Maxmind-Enrichment-Product-2013-07-14-09-25-42.csv\"\n",
      "    print(\"Lat,Lon\")\n",
      "    for node in normalized_nodes(csv_file):\n",
      "        print(node['LatLon'][0] + \",\" + node['LatLon'][1])\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lat,Lon\n",
        "33.7516,-84.3915"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34.0438,-118.2512\n",
        "38.0000,-97.0000\n",
        "38.0000,-97.0000\n",
        "32.8795,-96.9398\n",
        "38.9266,-77.3936\n",
        "51.5142,-0.0931\n",
        "33.7516,-84.3915\n",
        "34.0522,-118.2437\n",
        "32.8073,-117.1324\n",
        "41.8119,-87.6873\n",
        "36.0000,138.0000\n",
        "33.6119,-111.8906\n",
        "33.8359,-118.3406\n",
        "39.7157,-75.5281\n",
        "41.7444,-87.6054\n",
        "38.0000,-97.0000\n",
        "34.0533,-118.2549\n",
        "51.5142,-0.0931\n",
        "39.4899,-74.4773\n",
        "32.9299,-96.8353\n",
        "1.3667,103.8000\n",
        "34.0463,-117.9849"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38.0000,-97.0000\n",
        "32.7791,-96.8028\n",
        "40.2181,-111.6133\n",
        "40.7143,-74.0060\n",
        "31.0456,121.3997\n",
        "52.5000,5.7500\n",
        "46.0000,25.0000\n",
        "65.0000,-18.0000\n",
        "40.7391,-73.9826"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37.3887,-122.0188\n",
        "39.9612,-82.9988\n",
        "38.9266,-77.3936\n",
        "37.7731,-121.7515\n",
        "27.8959,-82.6720\n",
        "38.0000,-97.0000\n",
        "34.0530,-118.2642\n",
        "39.4899,-74.4773\n",
        "38.0000,-97.0000\n",
        "51.0000,9.0000\n",
        "22.2833,114.1500\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}